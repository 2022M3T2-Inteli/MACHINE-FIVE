# -*- coding: utf-8 -*-
"""Projeto5

Automatically generated by Colaboratory.



Original file is located at
    https://colab.research.google.com/drive/1v1DrwMH3kze_hqB0yL6pzyqU0WKARR7H

# 1.0 - Importando as bibliotecas

- Aqui importamos todas as bibliotecas que são utilizadas nos codigos abaixo.
"""

! pip install shap --quiet

import pandas as pd
from google.colab import drive
import numpy as np
import sklearn
from sklearn import preprocessing
import datetime
import plotly.express as px
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA 
from imblearn.over_sampling import RandomOverSampler, SMOTE
from sklearn.model_selection import train_test_split 
from sklearn import svm
from sklearn import metrics
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, f1_score, plot_roc_curve 
from sklearn.model_selection import cross_val_score, GridSearchCV,KFold
from sklearn.neighbors import KNeighborsClassifier 
from sklearn.naive_bayes import GaussianNB 
from pandas.core.arrays.interval import value_counts 
from sklearn.model_selection import RandomizedSearchCV
import shap


drive = drive.mount('/content/drive/')

"""## Carregando os dataframes"""

base = pd.read_excel("/content/drive/Shareddrives/grupo5_modulo3/Everymind | Grupo 5.xlsx", "Everymind")
reconhecimento = pd.read_excel("/content/drive/Shareddrives/grupo5_modulo3/Everymind | Grupo 5.xlsx", "Reconhecimento")
ambiente = pd.read_excel("/content/drive/Shareddrives/grupo5_modulo3/Everymind | Grupo 5.xlsx", "Ambiente de Trabalho 27.07")

"""# 2.0 - Análise dos Dados

## Explicação dos conjuntos



### Base
- **Matrícula**:: Matrícula do colaborador
- **Nome Completo**: Nome completo do colaborador - Dado codificado
- **Dt Admissao**: Data de admissão do colaborador na empresa
- **Dt Saida**: Data que o funcionário saiu da empresa - No caso de não ter saído é nulo (Dado já tratado)
- **Tipo Saida**: Tipo de saída do colaborador da empresa - No caso de não ter saído é nulo (Dado já tratado)
- **Cargo**: Cargo do colaborador na empresa
- **Salario Mês**: Salário mensal do colaborador
- **Dt Nascimento**: Data de nascimento do colaborador
- **Genero**: Gênero do colaborador - Masculino ou Feminino
- **Etnia**: Etnia declarada do colaborador - No caso de não ter sido especificada é "Não informada" (Dado já tratado)
- **Estado Civil**: Estado civil do colaborador
- **Escolaridade**: Grau de ensino do colaborador
- **Estado**: Estado de residência do colaborador
- **Cidade**: Cidade de residência do colaborador
- **Area**: Área que o colaborador trabalha na empresa
- **Idade**: Idade do colaborador com base na Dt Nascimento e no dia de hoje


---


### Reconhecimento
- **Matricula**: Matrícula do colaborador
- **Codinome**: Nome completo do colaborador - Dado codificado
- **Situação**: Situação do colaborador na empresa - Afastado, Ativo, ou Desligado
- **Data de Admissão**: Data de admissão do colaborador na empresa
- **Data Vigência**: Data de vigência do colaborador com o respectivo salário ou cargo
- **Novo Cargo**: Novo cargo que o colaborador recebeu
- **Novo Salario**: Novo salário que o colaborador recebeu
- **Motivo**: Motivo da promoção ou do aumento
- **Alterou Função**: Se o colaborador mudou de função


---


### Ambiente
- **Divisao**: Divisão em que os colaboradores atuam
- **Pilar**: Pilar de avaliação dos colaboradores
- **Pontuação**: Pontuação dos colaboradors nesse pilar
- **Fator**: Fator de avaliação dos colaboradores
- **Pontuação.1**: Pontuação dos colaboradors nesse fator
- **Pergunta**: Pergunta respondida pelos colaboradores
- **Pulou**: Porcentagem de pessoas que pulou essa avaliação
- **Muito Insatisfeito**: Porcentagem de votos "Muito Insatisfeito"
- **Insatisfeito**: Porcentagem de votos "Insatisfeito"
- **Neutro**: Porcentagem de votos "Neutro"
- **Satisfeito**: Porcentagem de votos "Satisfeito"
- **Muito Satisfeito**: Porcentagem de votos "Muito Satisfeito"
- **Taxa de Confiabilidade**: Taxa de confiabilidade da resposta na pergunta
"""

base.head()

reconhecimento.head()

ambiente.head()

"""## Exploração dos dados

###2.1 Exploração gráfica
"""

baseHist = base
baseHist = baseHist.drop(['Matricula','Dt Admissao', 'Dt Saida','Dt Nascimento','Tipo Saida', 'Cargo','Dt Nascimento','Etnia','Nome Completo','Cidade'],axis=1)
def mostra_hist(coluna):
  hist = px.histogram(baseHist,x=coluna)
  hist.show()
for columns in baseHist:
  mostra_hist(columns)

baseHistR = reconhecimento
baseHistR = reconhecimento.drop(['Alterou Função', 'Codinome','Data de Admissão','Data Vigência','Situação'], axis=1) 
def mostra_hist(coluna):
  hist = px.histogram(baseHistR,x=coluna)
  hist.show()
for columns in baseHistR:
  mostra_hist(columns)

baseHistA = ambiente
baseHistA = ambiente.drop(['Pontuação.1','Pergunta','Pulou','Muito Insatisfeito', 'Insatisfeito','Neutro','Satisfeito','Muito Satisfeito','Taxa de Confiabilidade'], axis=1) 
def mostra_hist(coluna):
  hist = px.histogram(baseHistA,x=coluna)
  hist.show()
for columns in baseHistA:
  mostra_hist(columns)

df = base
fig = px.scatter(df, x="Tipo Saida", y="Cidade", color="Estado",
                 )
fig.show()

df = base
fig = px.scatter(df, x="Salario Mês", y="Area", color="Estado",
                 )
fig.show()

"""### 2.2 Tipo dos dados

Conferind tipos dos dados nas colunas da base, reconhecimento, e ambiente
"""

base.dtypes

reconhecimento.dtypes

"""Descrição dos dados numéricos até então"""

base.describe()

"""# 3.0 Tratamento / Pré processamento dos dados


---
- Retirar dados que não serão usados
- Limpeza de dados nulos 

- Criação de label e hot encoder para variáveis categóricas 
"""

base = base.drop(['Nome Completo'], axis=1)
base = base.drop(['Etnia'], axis=1)

"""## 3.0.1 Criação dos Data Frames de uso
- Criação de novos df com colunas que serão utilizadas pelos modelos

Remoção das colunas Cidade, Dt Admissao, Dt Saida, Dt Nascimento na tabela base
"""

baseN = base.drop(['Cidade','Dt Admissao','Dt Saida','Dt Nascimento'], axis=1)

"""Remoção de espaços vazios nos conteúdos das células"""

for i in baseN.select_dtypes(include = 'object').columns:
  baseN[i] = baseN[i].str.replace(' ','')

"""Remoção das colunas Alterou Função, Codinome, Data de Admissão, Data Vigência, Situação na tabela reconhecimento"""

rec = reconhecimento.drop(['Alterou Função', 'Codinome','Data de Admissão','Data Vigência','Situação'], axis=1)

"""Remoção das colunas Pontuação.1, Pergunta, Pulou, Muito Insatisfeito, Insatisfeito, Neutro, Satisfeito, Muito Satisfeito, Taxa de Confiabilidade"""

ambi = ambiente.drop(['Pontuação.1','Pergunta','Pulou','Muito Insatisfeito', 'Insatisfeito','Neutro','Satisfeito','Muito Satisfeito','Taxa de Confiabilidade'], axis=1)

"""## 3.0.2 Tratamento tabela df ambi
- O código abaixo trata os dados de

Substituição da vírgula (,) para ponto (.) para utilização como dado numérico decimal
"""

ambi['Pontuação'] = ambi['Pontuação'].str.replace(',','.').astype(float)

"""- Agrupamento da coluna Pontuação na tabela ambiente Pela Divisao
- Separação da média das pontuações
"""

amb1 = ambi.groupby('Divisao')['Pontuação'].apply(list)
ambDF = pd.DataFrame(amb1) 
ambDF['Pontuação'] = ambDF['Pontuação'].apply(lambda x:sum(x)/len(x))

"""Retornando Áreas na coluna Area"""

baseN['Area'].unique()

"""Hot encoding das Áreas na coluna Area
- 0: Áreas que não existem mais
"""

baseN['Area'] = baseN['Area'].replace('CPG&Retail',8.9)
baseN['Area'] = baseN['Area'].replace('Education',8.7)
baseN['Area'] = baseN['Area'].replace('Core&Industrias',8.7)
baseN['Area'] = baseN['Area'].replace('AMS',8.3)
baseN['Area'] = baseN['Area'].replace('MktCloud',8.6)
baseN['Area'] = baseN['Area'].replace('BAC',0)
baseN['Area'] = baseN['Area'].replace('Produtos',0)
baseN['Area'] = baseN['Area'].replace('Vendas',8.5)
baseN['Area'] = baseN['Area'].replace('Analytics',8.8)
baseN['Area'] = baseN['Area'].replace('Commerce',8.7)
baseN['Area'] = baseN['Area'].replace('BPM',8.7)
baseN['Area'] = baseN['Area'].replace('Diretoria',8.7)
baseN['Area'] = baseN['Area'].replace('PS',8.3)
baseN['Area'] = baseN['Area'].replace('Financeiro',8.7)
baseN['Area'] = baseN['Area'].replace('Integration',8.8)
baseN['Area'] = baseN['Area'].replace('People',8.9)
baseN['Area'] = baseN['Area'].replace('AgenciaDigital',8.6)
baseN['Area'] = baseN['Area'].replace('BestMinds',0)
baseN['Area'] = baseN['Area'].replace('Infraestrutura',8.7)

"""## 3.0.3Tratamento do df rec
- O código abaixo trata os dados de Novo salário e Motivo que serão integrados com os dados do df baseN

Agrupamento de linhas com mesmo valor de matricula e criação de uma lista com os valores do salário atrelado a aquela matricula
"""

rec1 = rec.groupby('Matricula')['Novo Salario'].apply(list)
rec1DF = pd.DataFrame(rec1)

"""Utilização do metodo apply junto a uma função anonima para retirar a média dos valores da lista"""

rec1DF['Novo Salario'] = rec1DF['Novo Salario'].apply(lambda x:sum(x)/len(x))

"""Agrupamento da coluna Motivo pela Matricula do colaborador"""

rec2 = rec.groupby('Matricula')['Motivo'].apply(list)
DFrec2 = pd.DataFrame()
DFrec2 = pd.concat([DFrec2,rec2],axis=1)

"""Conversão dos elementos da coluna Motivo em string """

DFrec2['Motivo'] = DFrec2['Motivo'].apply(lambda x: str(x))

"""Visualização de todas as combinações de mérito e promoção existente"""

set(DFrec2['Motivo'])

"""## 3.0.4 Criação da lógica para label encoder.
---
Para realizar o lebel encoder foi criado uma lógica levando em consideração mérito como peso 1 e promoção como peso 2 e que quando o valores se igualavam a promoção era considerada como maior peso. EX:
0 = 1 mérito, 1 = 2 Méritos, 2 = Promoção e assim por diante. Para o label encoder foi feito a trasnformação padrão para o label, contudo ele ordena em ordem alfabética e para arrumar essa ordenação foi codificado do 1 até 13 para letras que seriam posteriormente ordenados de acordo com a lógica do label. Assim quando mais de uma combinação possuia o mesmo resultado em peso passava por uma atribuição intermediária até chegar ao valor definitivo


"""

test = DFrec2

"""Criação do LabelEncoder"""

label = preprocessing.LabelEncoder()
test['Motivo'] = label.fit_transform(test['Motivo'])

"""Label encoding"""

test = test.replace(0,'z')
test = test.replace(1,'a')
test = test.replace(2,'b')
test = test.replace(3,'c')
test = test.replace(4,'d') 
test = test.replace(5,'e') 
test = test.replace(6,'f') 
test = test.replace(7,'g') 
test = test.replace(8,'h')  
test = test.replace(9,'i') 
test = test.replace(10,'j') 
test = test.replace(11,'k') 
test = test.replace(12,'l') 
test = test.replace(13,'m')

test = test.replace('j',0) 
test = test.replace('a',1)
test = test.replace(24,2)
test = test.replace('i', 18)
test = test.replace(18,3)
test = test.replace('f','m')
test = test.replace('m','z')
test = test.replace('z',4)
test = test.replace('e',23)
test = test.replace(23,5)
test = test.replace('b',17)
test = test.replace(17,6)
test = test.replace('h',21)
test = test.replace(21,16)
test = test.replace(16,7)
test = test.replace(22,8)
test = test.replace('g','l')
test = test.replace('l',9)
test = test.replace('c','k')
test = test.replace('k',10)
test = test.replace(19,11)
test = test.replace('g','l')
test = test.replace('d',14)
test = test.replace(14,12)
test = test.replace(20,15)
test = test.replace(15,13)

"""## 3.0.5 Criação do df recF 
- Junção do Novo salario e Motivo tratado 
"""

recF = pd.concat([rec1DF,test],axis=1)
recF = recF.reset_index()
recF =  recF.rename(columns={'index': 'Matricula'})

recF

"""## 3.1 - Modificação de dados

##Label Encoder

### 3.1.1 - Dicionário de dados



#### Legendas:



##### Escolaridade
- 0: Ensino médio incompleto
- 1: Ensino médio 
- 2: Técnico 
- 3: Superior Incompleto
- 4: Graduação
- 5: Pós- Graduação
- 6: Mestrado

##### Motivo
- 0 = 1 Mérito  
- 1 = 2 méritos 
- 2 = 1 promoção 
- 3 = 1 promoção 
- 4 = 2 meritos e 1 promoção 
- 5 = 2 promoções 
- 6 = 3 meritos e 1 promoção 
- 7 = 1 mérito e 2 promoções
- 8 = 3 promoções 
- 9 = 2 meritos e 2 promoções 
- 10 = 3 meritos e 2 promoções 
- 11 = 2 méritos e 3 promoções 
- 12 = 3 méritos e 3 promoções  
- 13 = 1 mérito 4 promoções
"""

baseN.columns

"""Label encoding da coluna Escolaridade da base"""

label = preprocessing.LabelEncoder()
baseN['Escolaridade '] = label.fit_transform(baseN['Escolaridade '])
baseN['Escolaridade '] = baseN['Escolaridade '].replace(0,'a')
baseN['Escolaridade '] = baseN['Escolaridade '].replace(1,'b')
baseN['Escolaridade '] = baseN['Escolaridade '].replace(2,'c')
baseN['Escolaridade '] = baseN['Escolaridade '].replace(3,'d')
baseN['Escolaridade '] = baseN['Escolaridade '].replace(4,'e')
baseN['Escolaridade '] = baseN['Escolaridade '].replace(5,'f')
baseN['Escolaridade '] = baseN['Escolaridade '].replace(6,'g')

baseN['Escolaridade '] = baseN['Escolaridade '].replace('b',0)
baseN['Escolaridade '] = baseN['Escolaridade '].replace('a',1)
baseN['Escolaridade '] = baseN['Escolaridade '].replace('g',2)
baseN['Escolaridade '] = baseN['Escolaridade '].replace('f',3)
baseN['Escolaridade '] = baseN['Escolaridade '].replace('c',4)
baseN['Escolaridade '] = baseN['Escolaridade '].replace('e',5)
baseN['Escolaridade '] = baseN['Escolaridade '].replace('d',6)

"""## Hot Encoder"""

hotGeral = pd.get_dummies(baseN, columns= ['Estado','Genero','Estado Civil','Cargo'])

"""### 3.1.2 - Eliminação das tabelas que não serão utilizadas"""

hotGeral = hotGeral.drop(['Matricula','Tipo Saida','Salario Mês','Escolaridade ','Idade','Area'], axis=1)
hotGeral = hotGeral.reset_index()

hotGeral.head()

"""## 3.1.4 - Tratamento de datas

Criação da coluna tempo de casa
"""

base['Dt Admissao'] = pd.to_datetime(base['Dt Admissao'])

houseTime = []
colaboratorStatus = []

for ind in base.index:
  if base['Dt Saida'][ind]:
    houseTime.append((base['Dt Saida'][ind] - base['Dt Admissao'][ind]).days)
  else:
    houseTime.append(0)

for i in base.index:
  if pd.isnull(base['Dt Saida'][i]):
    colaboratorStatus.append(1) # Nao saiu
  else:
    colaboratorStatus.append(0) # Saiu

base['Tempo de casa'], base['Situacao'] = houseTime, colaboratorStatus

base[base['Dt Saida'].notna()].sort_values('Tempo de casa', ascending=False).head()

"""## Tratamento de dados 
- Retirada da coluna tempo de casa e complementar valores nan com a mediana 
-Criação do df Geral com todas informações uteis
"""

tempoCasa = base['Tempo de casa']
mediana = tempoCasa.median()
tempoCasa.fillna(mediana, inplace=True)

tempoCasa.head()

Geral = pd.concat([baseN,tempoCasa],axis=1)
Geral = Geral.drop(['Tipo Saida'],axis=1)
Geral = pd.merge(recF, Geral,on='Matricula')
Geral = Geral.reset_index()
Geral.head()

"""## Separação da coluna situação e implementação no df Geral"""

Situacao = base['Situacao']
Situacao.value_counts()
Situacao = Situacao.reset_index()

Situacao.head()

Geral = pd.merge(Geral,Situacao,on='index')
Geral.head()

"""Seperação das colunas em variáveis"""

novo_salario = Geral['Novo Salario']
salario_mes = Geral['Salario Mês'] 
idade = Geral['Idade']
area = Geral['Area']
motivo = Geral['Motivo']
escol = Geral['Escolaridade ']
geral_Inativo = Geral[Geral['Situacao'] == 0]

geral_Inativo.head()

"""Criação dos gráficos:
1. Novo Salário por Quantidade
2. Motivo por Quantidade
3. Salário Mês por Quantidade
4. Novo Salário por Quantidade
5. Gênero por Quantidade
6. Estado Civil por Quantidade
7. Escolaridade por Quantidade
8. Idade por quantidade
9. Tempo de casa por Quantidade
"""

baseHistg = geral_Inativo
baseHistg = baseHistg.drop(['Situacao','index','Matricula','Cargo','Area'],axis=1)
def mostra_hist(coluna):
  hist = px.histogram(baseHistg,x=coluna)
  hist.show()
for columns in baseHistg:
  mostra_hist(columns)

df = geral_Inativo
fig = px.scatter(df, x="Area", y="Cargo", color="Estado",
                 )
fig.show()

df = geral_Inativo
fig = px.scatter(df, x="Motivo", y="Cargo", color="Estado",
                 )
fig.show()

"""# 3.2 - Categorização

## Normalização das variaveis numéricas 
- Salario Mês 
- Idade 
- Novo Salario 
- Tempo de casa
"""

snVars = ['Salario Mês','Idade','Novo Salario','Tempo de casa']
nDf = Geral[snVars]
nDf

"""StandardScaler no DataFrame"""

scaler = StandardScaler()
scaler.fit(nDf)

scaler.transform(nDf)

scaler.fit_transform(nDf)

nDf_norma = scaler.transform(nDf)

"""Criação do DataFrama nDfpad"""

nDfpad = pd.DataFrame(nDf_norma, columns = nDf.columns)
nDfpad.head()

hotRec = pd.merge(hotGeral,Geral,on='index')
hotRec = hotRec.drop(['index','Matricula', 'Novo Salario', 'Motivo', 'Cargo', 'Salario Mês',
       'Genero', 'Estado Civil', 'Escolaridade ', 'Estado', 'Idade',
       'Tempo de casa', 'Situacao','Area'], axis=1)
hotRec.head()

"""## Criação do df que será utilizado no modelo"""

mod = pd.concat([nDfpad,hotRec],axis=1)
mod = pd.concat([mod,motivo],axis=1)
mod = pd.concat([mod,escol],axis=1)
mod = pd.concat([mod,area],axis=1)

"""## Seleção dos Atributos: Feature Engineering"""

mod

sit = Geral['Situacao']

modC = pd.concat([mod,sit],axis=1)

"""##Aplicação da Matriz de correlação"""

matriz_corr = Geral.corr().style.background_gradient(cmap='coolwarm')
target_corr = matriz_corr.data['Situacao']
df_corr = target_corr.abs().sort_values(ascending=False).to_frame().style.background_gradient(cmap='coolwarm')
df_corr

matriz_corr = modC.corr().style.background_gradient(cmap='coolwarm')
target_corr = matriz_corr.data['Situacao']
df_corr = target_corr.abs().sort_values(ascending=False).to_frame().style.background_gradient(cmap='coolwarm')
df_corr

"""## Aplicação do PCA"""

pca = PCA(n_components=9)
pca_res = pca.fit_transform(mod)

pca_res

pca.explained_variance_ratio_

df = pd.DataFrame({'var': pca.explained_variance_ratio_,
             'PC':['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9']})

df

dfp = pd.DataFrame(pca_res)

dfp

"""## Aplicação do oversampler no df com PCA"""

x = dfp
y = Geral['Situacao']
ros = RandomOverSampler(random_state = 32)
X_ros_res, y_ros_res = ros.fit_resample(x, y)

"""## Aplicação do SMOTE"""

smote = SMOTE(random_state = 32)
x_smote_res, y_smote_res = smote.fit_resample(X_ros_res , y_ros_res )

"""## Separação de X e Y para o modelo"""

X = pd.DataFrame()

X = x_smote_res
Y = y_smote_res 
x_train, x_test, y_train, y_test = train_test_split(X,Y,
                                                    test_size = 0.3,
                                                    random_state = 2)

"""## Aplicação do modelo SVM"""

svmClf = svm.SVC(kernel='rbf', C = 1, gamma = 1, probability=True)
svmClf.fit(x_train, y_train)

y_pred = svmClf.predict(x_test)
y_trei = svmClf.predict(x_train)

"""## Aplicação da Validação cruzada"""

scores = cross_val_score(svmClf,X,Y,cv=5)

scores

y_true = y_test
cm = confusion_matrix(y_true, y_pred)
cm

disp = ConfusionMatrixDisplay(confusion_matrix=cm)

disp.plot()
plt.show()

"""Acurácia do modelo para os conjuntos de treino e teste"""

print('Acuracidade (treino): ', accuracy_score(y_train, y_trei))
print('Acuracidade (teste): ', accuracy_score(y_test, y_pred))

"""Precisão, Recall e F1 Score dos teste em relação à predição"""

print("Precision: ", metrics.precision_score(y_test, y_pred))
print("Recall: ", metrics.recall_score(y_test, y_pred))
f1 = f1_score(y_test, y_pred)
print('F1-Score: %f' % f1)

plot_roc_curve(svmClf,x_test, y_test)

"""Separação dos parâmetros a serem utilizados"""

parameters = {'C': [0.1, 1, 10, 100, 1000], 
              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],
              'kernel': ['rbf','sigmoid','poly','linear']}
               
modelGS = GridSearchCV(svmClf, parameters)

"""## Estabilidade dos dados utilizando Shap"""

shap.initjs()
explainer = shap.KernelExplainer(svmClf.predict_proba, x_train, link="logit")
shap_values = explainer.shap_values(x_test, nsamples=100)
shap.force_plot(explainer.expected_value[0], shap_values[0], x_test, link="logit")

"""
### Variação do random state"""

todos_acc_test_train = []
maior_acc = 0
menor_acc = 100
test_acc_diferentes_array = [-1]
random_state_array = []

for i in range(0, 500):
  svmClf = svm.SVC(kernel='rbf', C = 1, gamma = 1, probability=True)
  svmClf.fit(x_train, y_train)
  treino_e_teste = [svmClf.score( x_train, y_train), svmClf.score( x_test, y_test)]
  if maior_acc < svmClf.score(x_test, y_test):
    maior_acc = svmClf.score(x_test, y_test)
  if menor_acc > svmClf.score(x_test, y_test):
    menor_acc = svmClf.score(x_test, y_test)
  if svmClf.score(x_test, y_test) not in test_acc_diferentes_array:
    test_acc_diferentes_array.append(svmClf.score(x_test, y_test))
    random_state_array.append(i)
  if treino_e_teste not in todos_acc_test_train:
    todos_acc_test_train.append([treino_e_teste])

print('Todas as acurácias em formato [[acc_treino, acc_teste]]: ', todos_acc_test_train)
print('Menor acurácia: ', menor_acc)
print('Maior acurácia: ', maior_acc)
print('Diferentes acurácias para o teste: ', test_acc_diferentes_array)
print('Random_states para cada acurácia acima: ', random_state_array)

"""#Modelo KNN

Criação do modelo
"""

knn = KNeighborsClassifier(n_neighbors=7)
knn.fit(x_train, y_train.squeeze())

"""Criação das variáveis """

y_predk = knn.predict(x_test) 
y_probk = knn.predict_proba(x_test)

"""Acurácia do modelo para treino e teste"""

print('Acuracidade (treino): ', knn.score(x_train, y_train ))
print('Acuracidade (teste): ', knn.score(x_test, y_test ))

"""Matriz de confusão"""

y_truek = y_test
cmk = confusion_matrix(y_true, y_predk)
cmk

disp = ConfusionMatrixDisplay(confusion_matrix=cmk)

disp.plot()
plt.show()

"""Curva Roc"""

plot_roc_curve(knn,x_test, y_test)

"""#Naive Bayes

Criação do modelo
"""

gnb = GaussianNB()

y_predN = gnb.fit(x_train, y_train).predict(x_test)
accuracy_score(y_test, y_predN)

"""Acurácia para treino e teste"""

print('Acuracidade (treino): ', gnb.score( x_train, y_train ))
print('Acuracidade (teste): ', gnb.score( x_test, y_test ))

"""#Modelo GBC

Criação do modelo
"""

from sklearn.ensemble import GradientBoostingClassifier
gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0).fit(x_train, y_train)

"""Acurácia para treino e teste"""

print('Acuracidade (treino): ', gbc.score( x_train, y_train ))
print('Acuracidade (teste): ', gbc.score( x_test, y_test ))

mod.columns

base.columns

import plotly.express as px

df = px.data.tips()
fig = px.box(df, x=base['Salario Mês'], y=base["Idade"])
fig.show()


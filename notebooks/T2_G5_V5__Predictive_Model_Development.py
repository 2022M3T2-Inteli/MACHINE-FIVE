# -*- coding: utf-8 -*-
"""Projeto5

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QrcIMnqvtGdZhz2zwjUwhI0t8hgK2aEq

<img src="https://lh3.googleusercontent.com/fife/AAbDypBBWxJxGFw0c0kluBntqQbfJo9ngTo2p5u-SdXb9yesohDsUDr3WiBD8ZyHUE17bAGHC1TV7-IsePJoL01HDBQZp6yPRZFf7a6_KAPV9uRzQiihfXfCTzbJwYfHWokZBS4uvVttBXxQFtqgejTlB5BGxGe2RWGCIgxRYoVSHDV590c_-G_MMTc1cHbiNfizPCyXSHjvHDC-5LjzxsGa57mykYgyG1p801yD1uPgMK2hdXjoN2q5lNBP-aZLE-58VakVpjLc_YPudIfjAZzpmBsyXY_XxAHENwDyRsoQ1z_343RTnTXoqRCkqQHjUAnAG95LBPL7V9aA6AizPN4H4_csCTmy3OZSoD0zosHX5z_wGHhvQhw3erxxSJ4XEhecUFN-7dUAapYZURsvShFylUZeFb5YQccOAWLQFVFYJKiACViIbsIX5ne25kS8uRS86Bvv00TuwSdev3-5aJNr0ut2FJWIUQJQbLarMK2qsfF7D7CT92ideCipR-msaqe9e_sO8n2GR10ta33_ajEPzprjlcVlmn9vh9_Xh9olbdUUglVH0jcK1qzz44X2cGFppfiXjbcFfn49psGKmidnkFKd2BJFppJWtds7rkNsyHbAFq9B7WCRZhdxuMjtygTjhvK84jduT27vy6HWGKRjTzkNXYUWdeMVfpxE8IUxRtpW9fagc0SylwqvLuuS7a5xI-R_p16Fwpr7ECyKiJmluRGTy3MdIqslRzFSjUKk60-DfF0rfu_yD-dnStAWV4E-H01R2G_5PdK6wPAVMRiQUZd0JSmAjK7fyngh8izhQ3syKgUzl_plItULepRosvuJLRqONV4vZ93FPOr7DLKntcR8mHiQHqXQk1M5dccFZMaG1booeyHsaIhaOjGMa-Y6iTUH3AnvuXoJt0S2bs2TAIYITAg4BV5Rjr4Wt95i_S0H1Uaorlaz2tAzB0JgilVfyvYJ7-xZcom0p4x0r66ybbnLV1mZV4fXBc70jLMO0FzDChKJPyECinqkWBmVaqc3YCYPBbZdApxYyU_rH_4Tb7dqCSOgJDYAkMDc-qvwaMb7vg7GiXkovT8ymP7fjuF_b0ga9fbiUYrfsq9R-IDPCyxhOAFIMIl0i-LaWMTMAvWTbw_566jUGz5MNnZDlnqKEbqmpaVUQ0R5XKaGEWXBfhmamOrLj9jm4AnYT1YvhULFyqpX1WyUX9WzAI-QMY1Y0s6sH8ySXaZuRJQIggcyRYDVtzmFE0wixPaSomeod2YikT9eTdQ_HyZ8xLucPVP_wxihtuhe4a2Q3cxPcbelsyT-yGn3fWc4T8Y7H6s04fvMzjchTX_0QcXcZslO3RYMjP9uQhFcXyzfcOSzlDPBYL6XvyWqlPO9yZ-ZZJL7ppoEQA3D91Jh8zlir5m4IOGV61w4csuVSAGIsgWzBeUcjUoyzMkR8nAOj1_BYSKcFsokpjA3wPKXXyzjw6GOiYMfZX1_Z1WKAhQsRKUJNJl3zSMteADsbnXT4Azo4QQaKnCq9iNFdg=w1920-h961" />

# Machine five 

## Integrantes: 
- Felipe Leão
- Igor Garcia
- Marcelo Feitosa
- Michel Mansur 
- Rodrigo Campos
- Vinícius Fernandes

<br>

## Definição do problema:
Apesar de sua grande atuação, a empresa busca uma retenção maior de seus funcionários e o core de talentos dentro da organização.
Consequentemente, o problema evidenciado pela empresa, se relaciona a alta taxa de rotatividade em diversas áreas da empresa, principalmente no cargo de desenvolvedor. Sendo algo que atinge diretamente a performance das equipes, além de demandar mais esforços da área de RH, com onboardings e entrevistas de emprego. Em termos descritivos, o cuidado com o talento de colaboradores na empresa é muito importante para o desenvolvimento da organização e de sua cultura. Isso é evidenciado, na dissertação de Márcia Mendonça da Faculdade Getúlio Vargas, que discorre sobre o diferencial das competências exercidas por alguns profissionais, e sua difícil aquisição no mercado de trabalho. 
A partir dessas informações, podemos afirmar que a inconsistência apresentada na retenção de funcionários a longo prazo, é algo que está impedindo a empresa de seguir sustentavelmente para um futuro melhor. Dito isso, a vontade de melhorar essa taxa de turnover é iminente. Para solução desse problema, a área de RH e a liderança não sabia especificamente quais seriam os pontos de atuação e como prever a saída de alguns funcionários. Sendo assim, o modelo vai facilitar muito a atuação desses times. Esses serão responsáveis por criar novas estratégias de atuação a partir dos dados, relatórios e predição segmentada pelo modelo criado.

# 1.0 - Importando as bibliotecas
---

Aqui importamos todas as bibliotecas que são utilizadas nos codigos abaixo.
"""

! pip install shap numpy==1.20

import pandas as pd
from google.colab import drive
import numpy as np
import sklearn
from sklearn import preprocessing
import datetime
import plotly.express as px
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA 
from imblearn.over_sampling import RandomOverSampler, SMOTE
from sklearn.model_selection import train_test_split 
from sklearn import svm
from sklearn import metrics
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, f1_score, plot_roc_curve 
from sklearn.model_selection import cross_val_score, GridSearchCV,KFold
from sklearn.neighbors import KNeighborsClassifier 
from sklearn.naive_bayes import GaussianNB 
from pandas.core.arrays.interval import value_counts 
from sklearn.model_selection import RandomizedSearchCV
import shap
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.preprocessing import PowerTransformer
import seaborn as sns 

drive = drive.mount('/content/drive/')

"""##1.1 - Carregando os dataframes"""

base = pd.read_excel("/content/drive/Shareddrives/grupo5_modulo3/Everymind | Grupo 5.xlsx", "Everymind")
reconhecimento = pd.read_excel("/content/drive/Shareddrives/grupo5_modulo3/Everymind | Grupo 5.xlsx", "Reconhecimento")
ambiente = pd.read_excel("/content/drive/Shareddrives/grupo5_modulo3/Everymind | Grupo 5.xlsx", "Ambiente de Trabalho 27.07")

"""# 2.0 - Entendimento dos dados
---

##2.1 -  Explicação dos conjuntos



### Base
- **Matrícula**:: Matrícula do colaborador
- **Nome Completo**: Nome completo do colaborador - Dado codificado
- **Dt Admissao**: Data de admissão do colaborador na empresa
- **Dt Saida**: Data que o funcionário saiu da empresa - No caso de não ter saído é nulo (Dado já tratado)
- **Tipo Saida**: Tipo de saída do colaborador da empresa - No caso de não ter saído é nulo (Dado já tratado)
- **Cargo**: Cargo do colaborador na empresa
- **Salario Mês**: Salário mensal do colaborador
- **Dt Nascimento**: Data de nascimento do colaborador
- **Genero**: Gênero do colaborador - Masculino ou Feminino
- **Etnia**: Etnia declarada do colaborador - No caso de não ter sido especificada é "Não informada" (Dado já tratado)
- **Estado Civil**: Estado civil do colaborador
- **Escolaridade**: Grau de ensino do colaborador
- **Estado**: Estado de residência do colaborador
- **Cidade**: Cidade de residência do colaborador
- **Area**: Área que o colaborador trabalha na empresa
- **Idade**: Idade do colaborador com base na Dt Nascimento e no dia de hoje


---


### Reconhecimento
- **Matricula**: Matrícula do colaborador
- **Codinome**: Nome completo do colaborador - Dado codificado
- **Situação**: Situação do colaborador na empresa - Afastado, Ativo, ou Desligado
- **Data de Admissão**: Data de admissão do colaborador na empresa
- **Data Vigência**: Data de vigência do colaborador com o respectivo salário ou cargo
- **Novo Cargo**: Novo cargo que o colaborador recebeu
- **Novo Salario**: Novo salário que o colaborador recebeu
- **Motivo**: Motivo da promoção ou do aumento
- **Alterou Função**: Se o colaborador mudou de função


---


### Ambiente
- **Divisao**: Divisão em que os colaboradores atuam
- **Pilar**: Pilar de avaliação dos colaboradores
- **Pontuação**: Pontuação dos colaboradors nesse pilar
- **Fator**: Fator de avaliação dos colaboradores
- **Pontuação.1**: Pontuação dos colaboradors nesse fator
- **Pergunta**: Pergunta respondida pelos colaboradores
- **Pulou**: Porcentagem de pessoas que pulou essa avaliação
- **Muito Insatisfeito**: Porcentagem de votos "Muito Insatisfeito"
- **Insatisfeito**: Porcentagem de votos "Insatisfeito"
- **Neutro**: Porcentagem de votos "Neutro"
- **Satisfeito**: Porcentagem de votos "Satisfeito"
- **Muito Satisfeito**: Porcentagem de votos "Muito Satisfeito"
- **Taxa de Confiabilidade**: Taxa de confiabilidade da resposta na pergunta
"""

base.head()

reconhecimento.head()

ambiente.head()

"""##2.2 - Exploração gráfica

Criação de um histograma para cada coluna dos dados brutos

Primeiro do data frame base
"""

baseHist = base
baseHist = baseHist.drop(['Matricula','Dt Admissao', 'Dt Saida','Dt Nascimento','Tipo Saida', 'Cargo','Dt Nascimento','Etnia','Nome Completo','Cidade'],axis=1)
def mostra_hist(coluna):
  hist = px.histogram(baseHist,x=coluna)
  hist.show()
for columns in baseHist:
  mostra_hist(columns)

"""Seguidamente do data frame reconhecimento"""

baseHistR = reconhecimento
baseHistR = reconhecimento.drop(['Alterou Função', 'Codinome','Data de Admissão','Data Vigência','Situação'], axis=1) 
def mostra_hist(coluna):
  hist = px.histogram(baseHistR,x=coluna)
  hist.show()
for columns in baseHistR:
  mostra_hist(columns)

"""Seguidamente do data frame ambiente"""

baseHistA = ambiente
baseHistA = ambiente.drop(['Pontuação.1','Pergunta','Pulou','Muito Insatisfeito', 'Insatisfeito','Neutro','Satisfeito','Muito Satisfeito','Taxa de Confiabilidade'], axis=1) 
def mostra_hist(coluna):
  hist = px.histogram(baseHistA,x=coluna)
  hist.show()
for columns in baseHistA:
  mostra_hist(columns)

"""Gráfico de dispersão de Tipo Saida X Cidade de dados brutos"""

df = base
fig = px.scatter(df, x="Tipo Saida", y="Cidade", color="Estado",
                 )
fig.show()

"""Gráfico de dispersão de Salario Mês X Area de dados brutos"""

df = base
fig = px.scatter(df, x="Salario Mês", y="Area", color="Estado",
                 )
fig.show()

"""## 2.3 - Tipo dos dados

Conferindo tipos dos dados nas colunas da base, reconhecimento, e ambiente
"""

base.dtypes

reconhecimento.dtypes

"""Descrição dos dados numéricos até então"""

base.describe()

"""# 3.0 Tratamento / Pré processamento dos dados


---
- Retirar dados que não serão usados
- Limpeza de dados nulos 

- Criação de label e hot encoder para variáveis categóricas 
"""

base = base.drop(['Nome Completo'], axis=1)
base = base.drop(['Etnia'], axis=1)

"""## 3.1 - Criação dos Data Frames de uso
- Criação de novos df com colunas que serão utilizadas pelos modelos

Remoção das colunas Cidade, Dt Admissao, Dt Saida, Dt Nascimento na tabela base
"""

baseN = base.drop(['Cidade','Dt Admissao','Dt Saida','Dt Nascimento'], axis=1)

"""Remoção de espaços vazios nos conteúdos das células"""

for i in baseN.select_dtypes(include = 'object').columns:
  baseN[i] = baseN[i].str.replace(' ','')

"""Remoção das colunas Alterou Função, Codinome, Data de Admissão, Data Vigência, Situação na tabela reconhecimento"""

rec = reconhecimento.drop(['Alterou Função', 'Codinome','Data de Admissão','Data Vigência','Situação'], axis=1)

"""Remoção das colunas Pontuação.1, Pergunta, Pulou, Muito Insatisfeito, Insatisfeito, Neutro, Satisfeito, Muito Satisfeito, Taxa de Confiabilidade"""

ambi = ambiente.drop(['Pontuação.1','Pergunta','Pulou','Muito Insatisfeito', 'Insatisfeito','Neutro','Satisfeito','Muito Satisfeito','Taxa de Confiabilidade'], axis=1)

"""## 3.2 - Tratamento tabela df ambi

Substituição da vírgula (,) para ponto (.) para utilização como dado numérico decimal
"""

ambi['Pontuação'] = ambi['Pontuação'].str.replace(',','.').astype(float)

"""- Agrupamento da coluna Pontuação na tabela ambiente Pela Divisao
- Separação da média das pontuações
"""

amb1 = ambi.groupby('Divisao')['Pontuação'].apply(list)
ambDF = pd.DataFrame(amb1) 
ambDF['Pontuação'] = ambDF['Pontuação'].apply(lambda x:sum(x)/len(x))

"""Retornando Áreas na coluna Area"""

baseN['Area'].unique()

"""Hot encoding das Áreas na coluna Area
- 0: Áreas que não existem mais
"""

baseN['Area'] = baseN['Area'].replace('CPG&Retail',8.9)
baseN['Area'] = baseN['Area'].replace('Education',8.7)
baseN['Area'] = baseN['Area'].replace('Core&Industrias',8.7)
baseN['Area'] = baseN['Area'].replace('AMS',8.3)
baseN['Area'] = baseN['Area'].replace('MktCloud',8.6)
baseN['Area'] = baseN['Area'].replace('BAC',0)
baseN['Area'] = baseN['Area'].replace('Produtos',0)
baseN['Area'] = baseN['Area'].replace('Vendas',8.5)
baseN['Area'] = baseN['Area'].replace('Analytics',8.8)
baseN['Area'] = baseN['Area'].replace('Commerce',8.7)
baseN['Area'] = baseN['Area'].replace('BPM',8.7)
baseN['Area'] = baseN['Area'].replace('Diretoria',8.7)
baseN['Area'] = baseN['Area'].replace('PS',8.3)
baseN['Area'] = baseN['Area'].replace('Financeiro',8.7)
baseN['Area'] = baseN['Area'].replace('Integration',8.8)
baseN['Area'] = baseN['Area'].replace('People',8.9)
baseN['Area'] = baseN['Area'].replace('AgenciaDigital',8.6)
baseN['Area'] = baseN['Area'].replace('BestMinds',0)
baseN['Area'] = baseN['Area'].replace('Infraestrutura',8.7)

"""## 3.3 - Tratamento do df rec
- O código abaixo trata os dados de Novo salário e Motivo que serão integrados com os dados do df baseN

Agrupamento de linhas com mesmo valor de matricula e criação de uma lista com os valores do salário atrelado a aquela matricula
"""

rec1 = rec.groupby('Matricula')['Novo Salario'].apply(list)
rec1DF = pd.DataFrame(rec1)

"""Utilização do metodo apply junto a uma função anonima para retirar a média dos valores da lista"""

rec1DF['Novo Salario'] = rec1DF['Novo Salario'].apply(lambda x:sum(x)/len(x))

"""Agrupamento da coluna Motivo pela Matricula do colaborador"""

rec2 = rec.groupby('Matricula')['Motivo'].apply(list)
DFrec2 = pd.DataFrame()
DFrec2 = pd.concat([DFrec2,rec2],axis=1)

"""Conversão dos elementos da coluna Motivo em string """

DFrec2['Motivo'] = DFrec2['Motivo'].apply(lambda x: str(x))

"""Visualização de todas as combinações de mérito e promoção existente"""

set(DFrec2['Motivo'])

"""## 3.4 - Criação da lógica para label encoder.
---
Para realizar o lebel encoder foi criado uma lógica levando em consideração mérito como peso 1 e promoção como peso 2 e que quando o valores se igualavam a promoção era considerada como maior peso. EX:
0 = 1 mérito, 1 = 2 Méritos, 2 = Promoção e assim por diante. Para o label encoder foi feito a trasnformação padrão para o label, contudo ele ordena em ordem alfabética e para arrumar essa ordenação foi codificado do 1 até 13 para letras que seriam posteriormente ordenados de acordo com a lógica do label. Assim quando mais de uma combinação possuia o mesmo resultado em peso passava por uma atribuição intermediária até chegar ao valor definitivo


"""

test = DFrec2

"""Criação do LabelEncoder"""

label = preprocessing.LabelEncoder()
test['Motivo'] = label.fit_transform(test['Motivo'])

"""Label encoding"""

test = test.replace(0,'z')
test = test.replace(1,'a')
test = test.replace(2,'b')
test = test.replace(3,'c')
test = test.replace(4,'d') 
test = test.replace(5,'e') 
test = test.replace(6,'f') 
test = test.replace(7,'g') 
test = test.replace(8,'h')  
test = test.replace(9,'i') 
test = test.replace(10,'j') 
test = test.replace(11,'k') 
test = test.replace(12,'l') 
test = test.replace(13,'m')

test = test.replace('j',0) 
test = test.replace('a',1)
test = test.replace(24,2)
test = test.replace('i', 18)
test = test.replace(18,3)
test = test.replace('f','m')
test = test.replace('m','z')
test = test.replace('z',4)
test = test.replace('e',23)
test = test.replace(23,5)
test = test.replace('b',17)
test = test.replace(17,6)
test = test.replace('h',21)
test = test.replace(21,16)
test = test.replace(16,7)
test = test.replace(22,8)
test = test.replace('g','l')
test = test.replace('l',9)
test = test.replace('c','k')
test = test.replace('k',10)
test = test.replace(19,11)
test = test.replace('g','l')
test = test.replace('d',14)
test = test.replace(14,12)
test = test.replace(20,15)
test = test.replace(15,13)

"""## 3.5 - Criação do df recF 
- Junção do Novo salario e Motivo tratado 
"""

recF = pd.concat([rec1DF,test],axis=1)
recF = recF.reset_index()
recF =  recF.rename(columns={'index': 'Matricula'})

recF

"""## 3.6 - Modificação de dados

###3.6.1 - Label Encoder

#### 3.6.1.1 - Dicionário de dados



##### Legendas:



###### Escolaridade
- 0: Ensino médio incompleto
- 1: Ensino médio 
- 2: Técnico 
- 3: Superior Incompleto
- 4: Graduação
- 5: Pós- Graduação
- 6: Mestrado

###### Motivo
- 0 = 1 Mérito  
- 1 = 2 méritos 
- 2 = 1 promoção 
- 3 = 1 promoção 
- 4 = 2 meritos e 1 promoção 
- 5 = 2 promoções 
- 6 = 3 meritos e 1 promoção 
- 7 = 1 mérito e 2 promoções
- 8 = 3 promoções 
- 9 = 2 meritos e 2 promoções 
- 10 = 3 meritos e 2 promoções 
- 11 = 2 méritos e 3 promoções 
- 12 = 3 méritos e 3 promoções  
- 13 = 1 mérito 4 promoções

Label encoding da coluna Escolaridade da base
"""

label = preprocessing.LabelEncoder()
baseN['Escolaridade '] = label.fit_transform(baseN['Escolaridade '])
baseN['Escolaridade '] = baseN['Escolaridade '].replace(0,'a')
baseN['Escolaridade '] = baseN['Escolaridade '].replace(1,'b')
baseN['Escolaridade '] = baseN['Escolaridade '].replace(2,'c')
baseN['Escolaridade '] = baseN['Escolaridade '].replace(3,'d')
baseN['Escolaridade '] = baseN['Escolaridade '].replace(4,'e')
baseN['Escolaridade '] = baseN['Escolaridade '].replace(5,'f')
baseN['Escolaridade '] = baseN['Escolaridade '].replace(6,'g')

baseN['Escolaridade '] = baseN['Escolaridade '].replace('b',0)
baseN['Escolaridade '] = baseN['Escolaridade '].replace('a',1)
baseN['Escolaridade '] = baseN['Escolaridade '].replace('g',2)
baseN['Escolaridade '] = baseN['Escolaridade '].replace('f',3)
baseN['Escolaridade '] = baseN['Escolaridade '].replace('c',4)
baseN['Escolaridade '] = baseN['Escolaridade '].replace('e',5)
baseN['Escolaridade '] = baseN['Escolaridade '].replace('d',6)

"""###3.6.2 - Hot Encoder"""

hotGeral = pd.get_dummies(baseN, columns= ['Estado','Genero','Estado Civil','Cargo'])

"""#### 3.6.2.1 - Eliminação das tabelas que não serão utilizadas"""

hotGeral = hotGeral.drop(['Matricula','Tipo Saida','Salario Mês','Escolaridade ','Idade','Area'], axis=1)
hotGeral = hotGeral.reset_index()

hotGeral.head()

"""### 3.6.3 - Tratamento de datas

Criação da coluna tempo de casa
"""

base['Dt Admissao'] = pd.to_datetime(base['Dt Admissao'])

houseTime = []
colaboratorStatus = []

for ind in base.index:
  if base['Dt Saida'][ind]:
    houseTime.append((base['Dt Saida'][ind] - base['Dt Admissao'][ind]).days)
  else:
    houseTime.append(0)

for i in base.index:
  if pd.isnull(base['Dt Saida'][i]):
    colaboratorStatus.append(1) # Nao saiu
  else:
    colaboratorStatus.append(0) # Saiu

base['Tempo de casa'], base['Situacao'] = houseTime, colaboratorStatus

base[base['Dt Saida'].notna()].sort_values('Tempo de casa', ascending=False).head()

"""####3.6.3.1 - Tratamento de dados 
- Retirada da coluna tempo de casa e complementar valores nan com a mediana 
-Criação do df Geral com todas informações uteis
"""

tempoCasa = base['Tempo de casa']
mediana = tempoCasa.median()
tempoCasa.fillna(mediana, inplace=True)

tempoCasa.head()

Geral = pd.concat([baseN,tempoCasa],axis=1)
Geral = Geral.drop(['Tipo Saida'],axis=1)
Geral = pd.merge(recF, Geral,on='Matricula')
Geral = Geral.reset_index()
Geral.head()

"""####3.6.3.2 - Separação da coluna situação e implementação no df Geral"""

Situacao = base['Situacao']
Situacao.value_counts()
Situacao = Situacao.reset_index()

Situacao.head()

Geral = pd.merge(Geral,Situacao,on='index')
Geral.head()

"""Seperação das colunas em variáveis"""

novo_salario = Geral['Novo Salario']
salario_mes = Geral['Salario Mês'] 
idade = Geral['Idade']
area = Geral['Area']
motivo = Geral['Motivo']
escol = Geral['Escolaridade ']
geral_Inativo = Geral[Geral['Situacao'] == 0]

geral_Inativo.head()

"""Criação dos histogramas de inativos:
1. Novo Salário por Quantidade
2. Motivo por Quantidade
3. Salário Mês por Quantidade
4. Novo Salário por Quantidade
5. Gênero por Quantidade
6. Estado Civil por Quantidade
7. Escolaridade por Quantidade
8. Idade por quantidade
9. Tempo de casa por Quantidade
"""

baseHistg = geral_Inativo
baseHistg = baseHistg.drop(['Situacao','index','Matricula','Cargo','Area'],axis=1)
def mostra_hist(coluna):
  hist = px.histogram(baseHistg,x=coluna)
  hist.show()
for columns in baseHistg:
  mostra_hist(columns)

"""Gráfico de dispersão Area X Cargo de inativos:"""

df = geral_Inativo
fig = px.scatter(df, x="Area", y="Cargo", color="Estado",
                 )
fig.show()

"""Gráfico de dispersão Motivo X Cargo de inativos:"""

df = geral_Inativo
fig = px.scatter(df, x="Motivo", y="Cargo", color="Estado",
                 )
fig.show()

sns.lineplot(x=Geral['Motivo'], y=Geral['Salario Mês'], marker='o')
sns.lineplot(x=Geral['Motivo'], y=Geral['Novo Salario'], marker='o')   
plt.title('Comparação salário com niveis de promoções')
plt.show()

df = Geral
fig = px.bar(df, x="Cargo", y="Tempo de casa", color="Genero", barmode="group")
fig.show()

"""## 3.7 - Tratamento de dados

###3.7.1 - Normalização das variaveis numéricas 
- Salario Mês 
- Idade 
- Novo Salario 
- Tempo de casa
"""

snVars = ['Salario Mês','Idade','Novo Salario','Tempo de casa']
nDf = Geral[snVars]
nDf

"""StandardScaler no DataFrame"""

scaler = StandardScaler()
scaler.fit(nDf)

scaler.transform(nDf)

scaler.fit_transform(nDf)

nDf_norma = scaler.transform(nDf)

"""Criação do DataFrama nDfpad"""

nDfpad = pd.DataFrame(nDf_norma, columns = nDf.columns)
nDfpad.head()

hotRec = pd.merge(hotGeral,Geral,on='index')
hotRec = hotRec.drop(['index','Matricula', 'Novo Salario', 'Motivo', 'Cargo', 'Salario Mês',
       'Genero', 'Estado Civil', 'Escolaridade ', 'Estado', 'Idade',
       'Tempo de casa', 'Situacao','Area'], axis=1)
hotRec.head()

df = px.data.tips()
fig = px.box(df, x=Geral['Cargo'], y=Geral['Salario Mês'])
fig.show()

"""###3.7.2 - Criação do df que será utilizado no modelo"""

mod = pd.concat([nDfpad,hotRec],axis=1)
mod = pd.concat([mod,motivo],axis=1)
mod = pd.concat([mod,escol],axis=1)
mod = pd.concat([mod,area],axis=1)

"""###3.7.3 - Seleção dos Atributos: Feature Engineering"""

mod.head()

"""Criação de histograma específico para cargo de DevJR sobre ativos e inativos 
- 0 inativo
- 1 ativo
"""

grafico = px.histogram(mod, x='Motivo', color='Cargo_DevJr')

grafico.show()

sit = Geral['Situacao']

"""###3.7.4 - Aplicação da Matriz de correlação

Criação de um novo data frame com a coluna situação e a variaveis escolidas para a matriz de correlação
"""

modC = pd.concat([mod,sit],axis=1)

matriz_corr = modC.corr().style.background_gradient(cmap='coolwarm')
target_corr = matriz_corr.data['Situacao']
df_corr = target_corr.abs().sort_values(ascending=False).to_frame().style.background_gradient(cmap='coolwarm')
df_corr

"""###3.7.5 - Aplicação do PCA

O PCA é um ferramenta muito utilizada devido sua função de reduzir a dimensionalidade dos dados, mesclando atributos de valores similares,  e assim os tornando mais fáceis de serem administrados.Com o seu uso, conseguimos ter uma maior aptidão na compreensão dos dados pelo modelo, e aumentamos a precisão, com base nas variáveis anteriormente segmentadas e de acordo com cada algoritmo.
"""

pca = PCA(n_components=9)
pca_res = pca.fit_transform(mod)

pca_res

pca.explained_variance_ratio_

df = pd.DataFrame({'var': pca.explained_variance_ratio_,
             'PC':['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9']})

df

dfp = pd.DataFrame(pca_res)

dfp

"""### 3.7.6 - Aplicação do oversampler no df com PCA"""

x = dfp
y = Geral['Situacao']
ros = RandomOverSampler(random_state = 32)
X_ros_res, y_ros_res = ros.fit_resample(x, y)

"""#4.0 - Modelagem
---

##4.1 - Separação de X e Y para o modelo
"""

X = pd.DataFrame()

X = X_ros_res
Y = y_ros_res
x_train, x_test, y_train, y_test = train_test_split(X,Y,
                                                    test_size = 0.3,
                                                    random_state = 2)

"""##4.2 - Aplicação do modelo SVM

O modelo escolido foi o SVM: Utilizamos o algoritmo de SVM para o treinamento e teste do nosso modelo também. Neste caso esse será o modelo que  iremos optar por para utilização, devido a seu ideal uso para bases menores de dados, sua eficácia trabalhando com várias variáveis, o nosso caso. Recorremos para sua utilização pela sua forma de classificação, que busca traçar uma reta e separar os dados nas classes estipuladas, o y, onde no nosso caso classificaremos como propenso a sair, ou propenso a continuar na empresa.
"""

svmClf = svm.SVC(kernel='rbf', C = 1, gamma = 1, probability=True)
svmClf.fit(x_train, y_train)

y_pred = svmClf.predict(x_test)
y_trei = svmClf.predict(x_train)

"""##4.3 - Métricas e hiper parâmetros"""

y_true = y_test
cm = confusion_matrix(y_true, y_pred)
cm

disp = ConfusionMatrixDisplay(confusion_matrix=cm)

disp.plot()
plt.show()

"""Acurácia do modelo para os conjuntos de treino e teste"""

print('Acuracidade (treino): ', accuracy_score(y_train, y_trei))
print('Acuracidade (teste): ', accuracy_score(y_test, y_pred))

"""Precisão, Recall e F1 Score dos teste em relação à predição"""

print("Precision: ", metrics.precision_score(y_test, y_pred))
print("Recall: ", metrics.recall_score(y_test, y_pred))
f1 = f1_score(y_test, y_pred)
print('F1-Score: %f' % f1)

plot_roc_curve(svmClf,x_test, y_test)

"""Separação dos parâmetros a serem utilizados"""

parameters = {'C': [0.1, 1, 10, 100, 1000], 
              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],
              'kernel': ['rbf','sigmoid','poly','linear']}
               
modelGS = GridSearchCV(svmClf, parameters)

"""##4.4 - Aplicação da Validação cruzada"""

scores = cross_val_score(svmClf,X,Y,cv=5)

scores

"""##4.5 - Estabilidade dos dados utilizando Shap"""

shap.initjs()
explainer = shap.KernelExplainer(svmClf.predict_proba, x_train, link="logit")
shap_values = explainer.shap_values(x_test, nsamples=100)
shap.force_plot(explainer.expected_value[0], shap_values[0], x_test, link="logit")

"""
##4.6 - Estabilidade do modelo com randow state"""

todos_acc_test_train = []
maior_acc = 0
menor_acc = 100
test_acc_diferentes_array = [-1]
random_state_array = []

for i in range(0, 500):
  svmClf = svm.SVC(kernel='rbf', C = 1, gamma = 1, probability=True)
  svmClf.fit(x_train, y_train)
  treino_e_teste = [svmClf.score( x_train, y_train), svmClf.score( x_test, y_test)]
  if maior_acc < svmClf.score(x_test, y_test):
    maior_acc = svmClf.score(x_test, y_test)
  if menor_acc > svmClf.score(x_test, y_test):
    menor_acc = svmClf.score(x_test, y_test)
  if svmClf.score(x_test, y_test) not in test_acc_diferentes_array:
    test_acc_diferentes_array.append(svmClf.score(x_test, y_test))
    random_state_array.append(i)
  if treino_e_teste not in todos_acc_test_train:
    todos_acc_test_train.append([treino_e_teste])

print('Todas as acurácias em formato [[acc_treino, acc_teste]]: ', todos_acc_test_train)
print('Menor acurácia: ', menor_acc)
print('Maior acurácia: ', maior_acc)
print('Diferentes acurácias para o teste: ', test_acc_diferentes_array)
print('Random_states para cada acurácia acima: ', random_state_array)

"""# 5.0 - Avaliação de outros modelos
---

##5.1 - Modelo KNN

Criação do modelo
"""

knn = KNeighborsClassifier(n_neighbors=7)
knn.fit(x_train, y_train.squeeze())

"""Criação das variáveis """

y_predk = knn.predict(x_test) 
y_probk = knn.predict_proba(x_test)

"""Acurácia do modelo para treino e teste"""

print('Acuracidade (treino): ', knn.score(x_train, y_train ))
print('Acuracidade (teste): ', knn.score(x_test, y_test ))

"""Matriz de confusão"""

y_truek = y_test
cmk = confusion_matrix(y_true, y_predk)
cmk

disp = ConfusionMatrixDisplay(confusion_matrix=cmk)

disp.plot()
plt.show()

"""Curva Roc"""

plot_roc_curve(knn,x_test, y_test)

"""Aplicando hiper parâmetros"""

# Calculando a acurácia dos modelos com diferentes valores de k
mean_acc = np.zeros(20)
for i in range(1,21):
    # Treinando o modelo e fazendo a predição 
    knn = KNeighborsClassifier(n_neighbors = i).fit(x_train,y_train)
    yhat= knn.predict(x_test)
    mean_acc[i-1] = metrics.accuracy_score(y_test, yhat)

mean_acc

grid_params = { 'n_neighbors' : [1,2,3,4,5,6],
               'weights' : ['uniform','distance'],
               'metric' : ['minkowski','euclidean','manhattan']}

gs = GridSearchCV(KNeighborsClassifier(), grid_params)

g_res = gs.fit(x_train, y_train)
display(g_res.best_score_)
display(g_res.best_params_)

"""##5.2 - Naive Bayes

Criação do modelo
"""

gnb = GaussianNB()

y_predN = gnb.fit(x_train, y_train).predict(x_test)
accuracy_score(y_test, y_predN)

"""Acurácia para treino e teste"""

print('Acuracidade (treino): ', gnb.score( x_train, y_train ))
print('Acuracidade (teste): ', gnb.score( x_test, y_test ))

"""Aplicando hiper parâmetros"""

np.logspace(0,-9, num=10)
params_NB = {'var_smoothing': np.logspace(0,-10, num=100)}
cv_method = RepeatedStratifiedKFold(n_splits=5, 
                                    n_repeats=3, 
                                    random_state=999)

gs_NB = GridSearchCV(estimator=gnb, 
                     param_grid=params_NB, 
                     cv=cv_method,
                     verbose=1, 
                     scoring='accuracy')

Data_transformed = PowerTransformer().fit_transform(x_test)

gs_NB.fit(Data_transformed, y_test)

display(gs_NB.best_params_)
display(gs_NB.best_score_)

"""## 5.3 - Modelo GBC

Criação do modelo
"""

gbc = GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
  learning_rate=0.4, loss='deviance', max_depth=8,
  max_features='sqrt', max_leaf_nodes=None,
  min_impurity_decrease=0.3,min_samples_leaf=3, min_samples_split=5,
  min_weight_fraction_leaf=0.0, n_estimators=180,
  n_iter_no_change=None,random_state=5910, subsample=0.9, tol=0.0001,
  validation_fraction=0.1, verbose=0,
  warm_start=False).fit(x_train, y_train)

"""Acurácia para treino e teste"""

print('Acuracidade (treino): ', gbc.score( x_train, y_train ))
print('Acuracidade (teste): ', gbc.score( x_test, y_test ))

"""#6.0 - Pycaret"""

#  ! pip install pycaret[full] numpy==1.20 --quiet

# import pycaret
# from pycaret.classification import *
# from pycaret.utils import enable_colab
# enable_colab()

# reg = setup(data = Geral, target = 'Situacao')

# compare_models()

# compare_models(n_select = 3, sort='F1')